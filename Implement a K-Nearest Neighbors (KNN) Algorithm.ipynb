{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f211ee4a-770f-46d2-9f71-7e65fba9cc62",
   "metadata": {},
   "source": [
    "# 1. Implement the K-nearest neighbors (KNN) algorithm from scratch. Given a dataset and a query point, classify the query based on the majority label of its k-nearestneighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8e7169-8bae-4ff6-b8c6-24641ba3b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query point [1.5, 1.5] is classified as: B\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(point1)):\n",
    "        distance += (point1[i] - point2[i]) ** 2\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def knn(dataset, query, k):\n",
    "    distances = []\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        features = data_point[:-1]  # all columns except the last one (label)\n",
    "        label = data_point[-1]  # last column is the label\n",
    "        distance = euclidean_distance(features, query)\n",
    "        distances.append((distance, label))\n",
    "    \n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    \n",
    "    k_nearest_labels = [distances[i][1] for i in range(k)]\n",
    "    \n",
    "    majority_vote = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "    \n",
    "    return majority_vote\n",
    "\n",
    "dataset = [\n",
    "    [2.5, 2.4, 'A'],\n",
    "    [0.5, 0.7, 'B'],\n",
    "    [2.2, 2.9, 'A'],\n",
    "    [1.9, 2.2, 'A'],\n",
    "    [3.1, 3.0, 'B'],\n",
    "    [2.3, 2.7, 'A'],\n",
    "    [1.0, 1.1, 'B'],\n",
    "    [1.5, 1.6, 'B'],\n",
    "    [1.1, 0.9, 'B'],\n",
    "]\n",
    "\n",
    "query_point = [1.5, 1.5]  \n",
    "k = 3  \n",
    "\n",
    "result = knn(dataset, query_point, k)\n",
    "print(f'The query point {query_point} is classified as: {result}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d655a7-c9a2-40f2-b04f-6d5e6d384edb",
   "metadata": {},
   "source": [
    "# 2. Problem: Implement logistic regression in Python. Include training using gradient descent,and calculate class probabilities for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709f9dfd-8c9a-4c86-8285-f0920e7ef84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, weights):\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, weights))\n",
    "    cost = -(1/m) * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, weights, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    for i in range(iterations):\n",
    "        predictions = sigmoid(np.dot(X, weights))\n",
    "        weights -= (learning_rate/m) * np.dot(X.T, (predictions - y))\n",
    "    return weights\n",
    "\n",
    "def logistic_regression(X, y, learning_rate, iterations):\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    weights = gradient_descent(X, y, weights, learning_rate, iterations)\n",
    "    return weights\n",
    "\n",
    "def predict(X, weights):\n",
    "    probabilities = sigmoid(np.dot(X, weights))\n",
    "    return [1 if p >= 0.5 else 0 for p in probabilities]\n",
    "\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "learning_rate = 0.1\n",
    "iterations = 1000\n",
    "\n",
    "weights = logistic_regression(X, y, learning_rate, iterations)\n",
    "predictions = predict(X, weights)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef66ea-9855-4081-9af5-f3602ff7cfa2",
   "metadata": {},
   "source": [
    "# 3. Problem: Write a function to perform matrix multiplication without using external librarieslike NumPy. Multiply two matrices representing weight matrices and input vectors in a neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7084c5d1-5369-4fa0-8c4f-93e389d0600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 30, 33]\n",
      "[61, 68, 75]\n",
      "[95, 106, 117]\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiply(matrix1, matrix2):\n",
    "    result = [[0 for _ in range(len(matrix2[0]))] for _ in range(len(matrix1))]\n",
    "    \n",
    "    for i in range(len(matrix1)):\n",
    "        for j in range(len(matrix2[0])):\n",
    "            for k in range(len(matrix2)):\n",
    "                result[i][j] += matrix1[i][k] * matrix2[k][j]\n",
    "    \n",
    "    return result\n",
    "\n",
    "matrix1 = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "\n",
    "matrix2 = [\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "]\n",
    "\n",
    "result = matrix_multiply(matrix1, matrix2)\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ac6fb-41d5-45d4-864c-b79089cc794e",
   "metadata": {},
   "source": [
    "#  4. Problem: Write a Python function that deep copies a neural network represented as a nested list of layers, where each layer contains weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa1b82f-1b8e-4aeb-aad3-cdde6f1505c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original network: [[[1.0, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]]]\n",
      "Copied network: [[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def deep_copy_neural_network(network):\n",
    "    return copy.deepcopy(network)\n",
    "\n",
    "neural_network = [\n",
    "    [[0.1, 0.2], [0.3, 0.4]],  \n",
    "    [[0.5, 0.6], [0.7, 0.8]] \n",
    "]\n",
    "\n",
    "copied_network = deep_copy_neural_network(neural_network)\n",
    "\n",
    "neural_network[0][0][0] = 1.0\n",
    "\n",
    "print(\"Original network:\", neural_network)\n",
    "print(\"Copied network:\", copied_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3898d5f-e821-43e6-ada8-0a7482b6da0d",
   "metadata": {},
   "source": [
    "#  5. Problem: Implement a moving average filter for a time series. Given a series of numbers and a window size, return the moving average of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a838541-4b46-486f-994e-34fd1dc77585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.0, 30.0, 40.0, 50.0, 60.0]\n"
     ]
    }
   ],
   "source": [
    "def moving_average(series, window_size):\n",
    "    moving_averages = []\n",
    "    for i in range(len(series) - window_size + 1):\n",
    "        window = series[i:i + window_size]\n",
    "        window_average = sum(window) / window_size\n",
    "        moving_averages.append(window_average)\n",
    "    return moving_averages\n",
    "\n",
    "# Example usage:\n",
    "time_series = [10, 20, 30, 40, 50, 60, 70]\n",
    "window_size = 3\n",
    "\n",
    "result = moving_average(time_series, window_size)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a204590-2c4a-4029-988c-c7865a7f0246",
   "metadata": {},
   "source": [
    "# 6. Problem: Write a Python function that applies the ReLU activation function to a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "996e35dc-e080-47e7-b713-341e01a6de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "def relu(inputs):\n",
    "    return [max(0, x) for x in inputs]\n",
    "\n",
    "# Example usage:\n",
    "input_values = [-3, -1, 0, 2, 5]\n",
    "\n",
    "output = relu(input_values)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8831e-21c3-4b87-be91-1bd92d3cc160",
   "metadata": {},
   "source": [
    "# 7. Problem: Implement gradient descent to train a linear regression model. Minimize the  mean squared error by adjusting the model weights iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbacfbab-17e1-4b1c-a234-93945e570fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-0.5270054   3.83912416]\n",
      "Final Mean Squared Error: 0.037364642472698294\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(X.shape[1])  \n",
    "    for _ in range(iterations):\n",
    "        predictions = X.dot(theta)  \n",
    "        errors = predictions - y  \n",
    "        gradient = (1/m) * X.T.dot(errors)  \n",
    "        theta -= learning_rate * gradient   \n",
    "    return theta\n",
    "\n",
    "def mean_squared_error(X, y, theta):\n",
    "    predictions = X.dot(theta)\n",
    "    error = predictions - y\n",
    "    mse = (1 / len(y)) * np.dot(error.T, error)\n",
    "    return mse\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]]) \n",
    "y = np.array([3, 7, 11, 15])  \n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "theta = gradient_descent(X, y, learning_rate, iterations)\n",
    "\n",
    "print(\"Weights:\", theta)\n",
    "\n",
    "mse = mean_squared_error(X, y, theta)\n",
    "print(\"Final Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2266f-f2c0-4a4c-b78c-85b1b35984e0",
   "metadata": {},
   "source": [
    "# 8. Problem: Write a Python function to perform one-hot encoding of categorical labels for a machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0723652-a070-478e-a2d7-7b82b21c8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    unique_labels = sorted(set(labels))  \n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}  \n",
    "    one_hot = np.zeros((len(labels), len(unique_labels)))  \n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot[i, label_map[label]] = 1  \n",
    "\n",
    "    return one_hot\n",
    "\n",
    "labels = ['cat', 'dog', 'fish', 'cat', 'dog']\n",
    "encoded_labels = one_hot_encode(labels)\n",
    "print(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e7cd9-487d-4678-9452-3de6b59961bb",
   "metadata": {},
   "source": [
    "# 9. Problem: Implement a Python function to calculate the cosine similarity between two vectors, which is often used in text similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11b340b-f576-4c02-a60f-e137a450c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2) \n",
    "    norm_vec1 = np.linalg.norm(vec1)  \n",
    "    norm_vec2 = np.linalg.norm(vec2)  \n",
    "    return dot_product / (norm_vec1 * norm_vec2)  \n",
    "\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 5, 6])\n",
    "\n",
    "similarity = cosine_similarity(vector1, vector2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872d261-62b7-41f7-8870-cb3edc86fef3",
   "metadata": {},
   "source": [
    "# 10. Problem: Given a trained neural network in Python, write a function to prune the network by removing neurons that have small or zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c2ff1b-c6c7-4f87-9d1d-c8383f1e0b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "[[0.1 0. ]\n",
      " [0.3 0.4]]\n",
      "[[0.  0.5]\n",
      " [0.6 0. ]]\n",
      "Layer 2:\n",
      "[[0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prune_neural_network(network, threshold=0.01):\n",
    "    pruned_network = []\n",
    "    \n",
    "    for layer in network:\n",
    "        pruned_layer = []\n",
    "        for weights in layer:\n",
    "            if np.abs(weights).max() > threshold:\n",
    "                pruned_layer.append(weights)\n",
    "        pruned_network.append(pruned_layer)\n",
    "    \n",
    "    return pruned_network\n",
    "\n",
    "neural_network = [\n",
    "    [np.array([[0.1, 0.0], [0.3, 0.4]]), np.array([[0.0, 0.5], [0.6, 0.0]])],  # First layer\n",
    "    [np.array([[0.2, 0.0]]), np.array([[0.0, 0.0]])]  # Second layer\n",
    "]\n",
    "\n",
    "pruned_network = prune_neural_network(neural_network, threshold=0.01)\n",
    "\n",
    "for i, layer in enumerate(pruned_network):\n",
    "    print(f\"Layer {i+1}:\")\n",
    "    for weights in layer:\n",
    "        print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352ceb0-0da9-45e3-9d1e-7d25b8d2555a",
   "metadata": {},
   "source": [
    "# 11. Problem: Implement a Python function to compute the confusion matrix given true and predicted labels for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a16ceec-26e8-4b61-8284-df254b2a2706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 1 0]\n",
      " [1 1 0]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    cm = np.zeros((len(classes), len(classes)), dtype=int)  \n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        cm[true, pred] += 1 \n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "y_true = np.array([0, 1, 2, 2, 0, 1, 0])  \n",
    "y_pred = np.array([0, 0, 2, 2, 0, 1, 1])  \n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66caff6-d91d-41da-9f01-db2a4123e5a9",
   "metadata": {},
   "source": [
    "# 12. Problem: Write a Python function that performs mini-batch gradient descent for optimizing the weights of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98483f0-3486-40be-8d79-c263a8c6267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights: [-0.99858268  2.20068112]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mini_batch_gradient_descent(X, y, theta, learning_rate=0.01, batch_size=32, iterations=1000):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for start in range(0, m, batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            predictions = X_batch.dot(theta)\n",
    "            errors = predictions - y_batch\n",
    "            gradient = (1 / batch_size) * X_batch.T.dot(errors)\n",
    "            theta -= learning_rate * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "y = np.array([2, 3, 5, 7, 11])\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 2\n",
    "iterations = 1000\n",
    "\n",
    "optimized_theta = mini_batch_gradient_descent(X, y, theta, learning_rate, batch_size, iterations)\n",
    "print(\"Optimized Weights:\", optimized_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb93da-d986-4cd8-b33f-28530f25960f",
   "metadata": {},
   "source": [
    "# 13. Problem: Implement k-means clustering from scratch. Given a dataset and the number of clusters k , return the cluster assignments for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b94de24-7c0a-4881-bf43-8bf41139857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Assignments: [1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def k_means(X, k, max_iterations=100):\n",
    "    m, n = X.shape\n",
    "    centroids = X[np.random.choice(m, k, replace=False)]\n",
    "    cluster_assignments = np.zeros(m)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        cluster_assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "        new_centroids = np.array([X[cluster_assignments == i].mean(axis=0) for i in range(k)])\n",
    "        \n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return cluster_assignments\n",
    "\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "k = 2\n",
    "assignments = k_means(X, k)\n",
    "print(\"Cluster Assignments:\", assignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc909f5-071d-480e-b114-9d8b09c65587",
   "metadata": {},
   "source": [
    "# 14. Problem: Implement a Python function to calculate the softmax of a list of numbers, which is used in multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "028360c0-e964-4f0f-b250-5f6e6b349349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Values: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  \n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "logits = [2.0, 1.0, 0.1]\n",
    "softmax_values = softmax(np.array(logits))\n",
    "print(\"Softmax Values:\", softmax_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728fa45-3763-46e5-8787-54fb2b62b8cb",
   "metadata": {},
   "source": [
    " # 15. Problem: Write a Python function to compute the TF-IDF (Term Frequency-InverseDocument Frequency) for a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812ec437-24b3-467b-8f21-635dea50ba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['and' 'are' 'cat' 'dog' 'friends' 'log' 'mat' 'on' 'sat' 'the']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.37420726 0.         0.         0.\n",
      "  0.49203758 0.37420726 0.37420726 0.58121064]\n",
      " [0.         0.         0.         0.37420726 0.         0.49203758\n",
      "  0.         0.37420726 0.37420726 0.58121064]\n",
      " [0.42439575 0.42439575 0.32276391 0.32276391 0.42439575 0.\n",
      "  0.         0.         0.         0.50130994]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix.toarray(), vectorizer.get_feature_names_out()\n",
    "\n",
    "documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "tfidf_values, feature_names = compute_tfidf(documents)\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df043470-21b6-4d48-95d5-a09bdecb3408",
   "metadata": {},
   "source": [
    "# 16. Problem: Implement an algorithm to find the principal components of a dataset usingsingular value decomposition (SVD), which is a core part of PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e4c821-4978-44c8-9cd7-1f52354aa48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      " [[-0.24356016]\n",
      " [ 0.52290258]\n",
      " [-0.29187014]\n",
      " [-0.08066321]\n",
      " [-0.49296275]\n",
      " [-0.26855801]\n",
      " [ 0.02915456]\n",
      " [ 0.3366935 ]\n",
      " [ 0.128858  ]\n",
      " [ 0.36000563]]\n",
      "Singular Values:\n",
      " [3.3994484]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca_svd(X, n_components):\n",
    "    X_meaned = X - np.mean(X, axis=0)\n",
    "    U, S, Vt = np.linalg.svd(X_meaned, full_matrices=False)\n",
    "    return U[:, :n_components], S[:n_components], Vt[:n_components, :]\n",
    "\n",
    "X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n",
    "n_components = 1\n",
    "\n",
    "principal_components, singular_values, explained_variance = pca_svd(X, n_components)\n",
    "print(\"Principal Components:\\n\", principal_components)\n",
    "print(\"Singular Values:\\n\", singular_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5ff2b-339d-4858-b7ab-18a187cb74e6",
   "metadata": {},
   "source": [
    "# 17. Problem: Write a Python function to calculate the AUC-ROC score for a binary classification problem, given the true labels and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86bd8f02-ccbd-4b6a-a32c-1df44d992f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_auc_roc(y_true, y_scores):\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_scores = [0.1, 0.4, 0.35, 0.8]\n",
    "\n",
    "auc_roc = calculate_auc_roc(y_true, y_scores)\n",
    "print(\"AUC-ROC Score:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91691495-cf23-4245-a090-68ad8a8f59b5",
   "metadata": {},
   "source": [
    "# 18. Problem: Implement a Python function to apply dropout regularization to the neurons of agiven neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f979d721-86c0-4e39-89a5-0440e9e60b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Dropout:\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_dropout(layer_output, dropout_rate):\n",
    "    if dropout_rate < 0 or dropout_rate >= 1:\n",
    "        raise ValueError(\"Dropout rate must be between 0 and 1.\")\n",
    "    keep_prob = 1 - dropout_rate\n",
    "    mask = np.random.binomial(1, keep_prob, size=layer_output.shape)\n",
    "    return layer_output * mask / keep_prob\n",
    "\n",
    "layer_output = np.array([[0.5, 0.6], [0.2, 0.3], [0.8, 0.7]])\n",
    "dropout_rate = 0.5\n",
    "\n",
    "output_with_dropout = apply_dropout(layer_output, dropout_rate)\n",
    "print(\"Output with Dropout:\\n\", output_with_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998decd-cd9a-4c14-8423-33265f4c6997",
   "metadata": {},
   "source": [
    " # 19. Problem: Write a Python function to perform feature scaling using standardization (z-score normalization), which transforms features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d43fe11-cdb2-4cdd-b9ce-fdb11dbd4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Features:\n",
      " [[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_features(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    standardized_X = (X - mean) / std\n",
    "    return standardized_X\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "standardized_X = standardize_features(X)\n",
    "print(\"Standardized Features:\\n\", standardized_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81887fc7-3749-4368-aa19-63325cc3257d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
